import io
import re
import pandas as pd
import streamlit as st
import plotly.express as px
from typing import Dict, Any

# ========================
# 1. é é¢è¨­å®šèˆ‡å·¥å…·å‡½å¼
# ========================

st.set_page_config(
    page_title="TMSå‡ºè²¨é…é€æ•¸æ“šåˆ†æ",
    page_icon="ğŸ“Š",
    layout="wide"
)
st.title("ğŸ“Š TMSå‡ºè²¨é…é€æ•¸æ“šåˆ†æ")
st.caption("ä¸Šå‚³ Excel/CSV â†’ å‡ºè²¨é¡å‹ç­†æ•¸ï¼ˆéŠ…é‡é‡æ›ç®—å™¸ï¼‰â†’ é…é€é”äº¤ç‡ â†’ é…é€å€åŸŸåˆ†æ â†’ é…é€è£è¼‰åˆ†æ")

# --- æª”æ¡ˆä¸Šå‚³èˆ‡å¿«å– ---
@st.cache_data
def load_data(file_buffer: io.BytesIO, file_type: str) -> pd.DataFrame:
    """å¿«å–ä¸¦è¼‰å…¥ Excel æˆ– CSV æª”æ¡ˆã€‚"""
    if file_type == "csv":
        return pd.read_csv(file_buffer)
    elif file_type in ["xlsx", "xls"]:
        return pd.read_excel(file_buffer, engine="openpyxl")
    else:
        st.error("ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹ã€‚è«‹ä¸Šå‚³ .xlsx, .xls æˆ– .csv æª”æ¡ˆã€‚")
        return pd.DataFrame()

# --- æ¬„ä½è‡ªå‹•åµæ¸¬ ---
def _guess_col(cols: list, keywords: list) -> str | None:
    """å¾æ¬„ä½åˆ—è¡¨ä¸­çŒœæ¸¬ç¬¦åˆé—œéµå­—çš„æ¬„ä½åç¨±ã€‚"""
    for kw in keywords:
        for c in cols:
            if kw in str(c):
                return c
    return None

def guess_column_names(df_cols: list) -> Dict[str, str | None]:
    """æ‰¹é‡åµæ¸¬å¸¸ç”¨çš„æ¬„ä½åç¨±ï¼Œä¸¦å›å‚³å­—å…¸ã€‚"""
    col_map = {
        "ship_type": _guess_col(df_cols, ["å‡ºè²¨ç”³è«‹é¡å‹", "å‡ºè²¨é¡å‹", "é…é€é¡å‹", "é¡å‹"]),
        "due_date": _guess_col(df_cols, ["æŒ‡å®šåˆ°è²¨æ—¥æœŸ", "åˆ°è²¨æ—¥æœŸ", "æŒ‡å®šåˆ°è²¨", "åˆ°è²¨æ—¥"]),
        "sign_date": _guess_col(df_cols, ["å®¢æˆ¶ç°½æ”¶æ—¥æœŸ", "ç°½æ”¶æ—¥æœŸ", "ç°½æ”¶æ—¥", "å®¢æˆ¶ç°½æ”¶æ—¥æœŸ/æ™‚/åˆ†"]),
        "cust_id": _guess_col(df_cols, ["å®¢æˆ¶ç·¨è™Ÿ", "å®¢æˆ¶ä»£è™Ÿ", "å®¢ç·¨"]),
        "cust_name": _guess_col(df_cols, ["å®¢æˆ¶åç¨±", "å®¢å", "å®¢æˆ¶"]),
        "address": _guess_col(df_cols, ["åœ°å€", "æ”¶è²¨åœ°å€", "é€è²¨åœ°å€", "äº¤è²¨åœ°å€"]),
        "copper_ton": _guess_col(df_cols, ["éŠ…é‡é‡(å™¸)", "éŠ…é‡é‡(å™¸æ•¸)", "éŠ…å™¸", "éŠ…é‡é‡"]),
        "qty": _guess_col(df_cols, ["å‡ºè²¨æ•¸é‡", "æ•¸é‡", "å‡ºè²¨é‡"]),
        "ship_no": _guess_col(df_cols, ["å‡ºåº«å–®è™Ÿ", "å‡ºåº«å–®", "å‡ºåº«ç·¨è™Ÿ"]),
        "do_no": _guess_col(df_cols, ["DOè™Ÿ", "DO", "å‡ºè²¨å–®è™Ÿ", "äº¤è²¨å–®è™Ÿ"]),
        "item_desc": _guess_col(df_cols, ["æ–™è™Ÿèªªæ˜", "å“å", "å“åè¦æ ¼", "ç‰©æ–™èªªæ˜"]),
        "lot_no": _guess_col(df_cols, ["æ‰¹æ¬¡", "æ‰¹è™Ÿ", "Lot"]),
        "fg_net_ton": _guess_col(df_cols, ["æˆå“æ·¨é‡(å™¸)", "æˆå“æ·¨é‡(å™¸)", "æ·¨é‡(å™¸)"]),
        "fg_gross_ton": _guess_col(df_cols, ["æˆå“æ¯›é‡(å™¸)", "æˆå“æ¯›é‡(å™¸)", "æ¯›é‡(å™¸)"]),
        "status": _guess_col(df_cols, ["é€šçŸ¥å–®é …æ¬¡ç‹€æ…‹", "é …æ¬¡ç‹€æ…‹", "ç‹€æ…‹"]),
    }
    return col_map

# --- è³‡æ–™è™•ç†èˆ‡æ­£è¦åŒ– ---
def to_dt(series: pd.Series) -> pd.Series:
    """å°‡ Series è½‰æ›ç‚ºæ—¥æœŸæ™‚é–“æ ¼å¼ã€‚"""
    return pd.to_datetime(series, errors="coerce")

def extract_city(addr: str) -> str | None:
    """å¾åœ°å€å­—ä¸²ä¸­æ“·å–å°ç£ç¸£å¸‚ï¼ˆå°‡ã€è‡ºã€æ­£è¦åŒ–ç‚ºã€å°ã€ï¼‰ã€‚"""
    if pd.isna(addr):
        return None
    s = str(addr).strip().replace("è‡º", "å°")
    pattern = r"(å°åŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|å°ä¸­å¸‚|å°å—å¸‚|é«˜é›„å¸‚|åŸºéš†å¸‚|æ–°ç«¹å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©å¸‚|å˜‰ç¾©ç¸£|å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)"
    m = re.search(pattern, s)
    return m.group(1) if m else None

# ========================
# 2. æ ¸å¿ƒåˆ†æå‡½å¼
# ========================

def display_shipment_type_analysis(df: pd.DataFrame, col_map: Dict[str, str | None]):
    """â‘  é¡¯ç¤ºå‡ºè²¨é¡å‹ç­†æ•¸çµ±è¨ˆèˆ‡éŠ…é‡é‡ã€‚"""
    st.subheader("â‘  å‡ºè²¨é¡å‹ç­†æ•¸èˆ‡é‡é‡çµ±è¨ˆ")
    ship_type_col = col_map.get("ship_type")
    copper_ton_col = col_map.get("copper_ton")

    if not ship_type_col or ship_type_col not in df.columns:
        st.warning("æ‰¾ä¸åˆ°ã€å‡ºè²¨ç”³è«‹é¡å‹ã€æ¬„ä½ï¼Œè«‹ç¢ºèªä¸Šå‚³æª”æ¡ˆæ¬„åã€‚")
        return

    type_counts = df[ship_type_col].fillna("(ç©ºç™½)").value_counts(dropna=False).rename_axis("å‡ºè²¨é¡å‹").reset_index(name="ç­†æ•¸")
    total_rows = int(type_counts["ç­†æ•¸"].sum())

    merged_df = type_counts.copy()
    if copper_ton_col and copper_ton_col in df.columns:
        df["_copper_kg"] = pd.to_numeric(df[copper_ton_col], errors="coerce")
        copper_sums = df.groupby(ship_type_col)["_copper_kg"].sum(min_count=1).reset_index(name="éŠ…é‡é‡(kg)åˆè¨ˆ")
        merged_df = merged_df.merge(copper_sums, how="left", left_on="å‡ºè²¨é¡å‹", right_on=ship_type_col)
        merged_df["éŠ…é‡é‡(å™¸)åˆè¨ˆ"] = (merged_df["éŠ…é‡é‡(kg)åˆè¨ˆ"] / 1000).round(2)
        merged_df.drop(columns=[ship_type_col, "éŠ…é‡é‡(kg)åˆè¨ˆ"], inplace=True)
    else:
        merged_df["éŠ…é‡é‡(å™¸)åˆè¨ˆ"] = None

    k1, k2 = st.columns(2)
    k1.metric("ç¸½è³‡æ–™ç­†æ•¸", f"{total_rows:,}")
    total_copper_ton = merged_df["éŠ…é‡é‡(å™¸)åˆè¨ˆ"].sum() if "éŠ…é‡é‡(å™¸)åˆè¨ˆ" in merged_df.columns else 0.0
    k2.metric("ç¸½éŠ…é‡é‡(å™¸)", f"{total_copper_ton:,.2f}")

    c1, c2 = st.columns([1, 1])
    with c1:
        st.dataframe(merged_df, use_container_width=True)
        st.download_button(
            "ä¸‹è¼‰å‡ºè²¨é¡å‹çµ±è¨ˆ CSV",
            data=merged_df.to_csv(index=False).encode("utf-8-sig"),
            file_name="å‡ºè²¨é¡å‹_çµ±è¨ˆ.csv",
            mime="text/csv",
        )
    with c2:
        chart_choice = st.radio("åœ–è¡¨é¡å‹", ["é•·æ¢åœ–", "åœ“é¤…åœ–", "ç’°å½¢åœ–"], horizontal=True, key="chart1")
        fig = px.bar(type_counts, x="å‡ºè²¨é¡å‹", y="ç­†æ•¸") if chart_choice == "é•·æ¢åœ–" else px.pie(
            type_counts, names="å‡ºè²¨é¡å‹", values="ç­†æ•¸", hole=0.5 if chart_choice == "ç’°å½¢åœ–" else 0.0
        )
        st.plotly_chart(fig, use_container_width=True)

def display_delivery_performance(df: pd.DataFrame, col_map: Dict[str, str | None], exclude_swi_mask: pd.Series):
    """â‘¡ é¡¯ç¤ºé…é€é”äº¤ç‡åˆ†æã€‚"""
    st.subheader("â‘¡ é…é€é”äº¤ç‡åˆ†æï¼ˆä¸å«æ™‚åˆ†ç§’ï¼‰")
    due_date_col = col_map.get("due_date")
    sign_date_col = col_map.get("sign_date")
    cust_id_col = col_map.get("cust_id")
    cust_name_col = col_map.get("cust_name")

    if not due_date_col or not sign_date_col or due_date_col not in df.columns or sign_date_col not in df.columns:
        st.warning("æ‰¾ä¸åˆ°ã€æŒ‡å®šåˆ°è²¨æ—¥æœŸã€æˆ–ã€å®¢æˆ¶ç°½æ”¶æ—¥æœŸã€æ¬„ä½ã€‚")
        return

    due_day = to_dt(df[due_date_col]).dt.normalize()
    sign_day = to_dt(df[sign_date_col]).dt.normalize()
    valid_mask = due_day.notna() & sign_day.notna() & exclude_swi_mask
    on_time_mask = (sign_day <= due_day) & valid_mask

    total_valid = int(valid_mask.sum())
    on_time_count = int(on_time_mask.sum())
    rate = (on_time_count / total_valid * 100) if total_valid > 0 else 0.0

    late_mask = valid_mask & (sign_day > due_day)
    delay_days = (sign_day - due_day).dt.days
    late_df = pd.DataFrame({
        "å®¢æˆ¶ç·¨è™Ÿ": df[cust_id_col] if cust_id_col else None,
        "å®¢æˆ¶åç¨±": df[cust_name_col] if cust_name_col else None,
        "æŒ‡å®šåˆ°è²¨æ—¥æœŸ": due_day.dt.strftime("%Y-%m-%d"),
        "å®¢æˆ¶ç°½æ”¶æ—¥æœŸ": sign_day.dt.strftime("%Y-%m-%d"),
        "å»¶é²å¤©æ•¸": delay_days,
    })[late_mask].dropna(axis=1, how="all")

    avg_delay = late_df["å»¶é²å¤©æ•¸"].mean() if not late_df.empty else 0.0
    k1, k2, k3, k4 = st.columns(4)
    k1.metric("æœ‰æ•ˆç­†æ•¸", f"{total_valid:,}")
    k2.metric("æº–æ™‚äº¤ä»˜ç­†æ•¸", f"{on_time_count:,}")
    k3.metric("é”äº¤ç‡", f"{rate:.2f}%")
    k4.metric("å¹³å‡å»¶é²å¤©æ•¸", f"{avg_delay:.2f}")

    st.write("**æœªé”æ¨™æ¸…å–®**ï¼ˆå·²æ’é™¤SWI-å¯„åº«ï¼›åƒ…åˆ—ç°½æ”¶æ™šæ–¼æŒ‡å®šåˆ°è²¨è€…ï¼‰")
    st.dataframe(late_df, use_container_width=True)
    st.download_button(
        "ä¸‹è¼‰æœªé”æ¨™æ¸…å–® CSV",
        data=late_df.to_csv(index=False).encode("utf-8-sig"),
        file_name="æœªé”æ¨™æ¸…å–®.csv",
        mime="text/csv",
    )
    if cust_name_col and cust_name_col in df.columns:
        customer_perf = pd.DataFrame({
            "å®¢æˆ¶åç¨±": df[cust_name_col],
            "æ˜¯å¦æœ‰æ•ˆ": valid_mask,
            "æ˜¯å¦é²äº¤": late_mask,
        }).groupby("å®¢æˆ¶åç¨±").agg(
            æœ‰æ•ˆç­†æ•¸=("æ˜¯å¦æœ‰æ•ˆ", "sum"),
            æœªé”äº¤ç­†æ•¸=("æ˜¯å¦é²äº¤", "sum")
        )
        customer_perf = customer_perf[customer_perf["æœªé”äº¤ç­†æ•¸"] > 0].reset_index()
        customer_perf["æœªé”äº¤æ¯”ä¾‹(%)"] = (customer_perf["æœªé”äº¤ç­†æ•¸"] / customer_perf["æœ‰æ•ˆç­†æ•¸"] * 100).round(2)
        customer_perf = customer_perf.sort_values(["æœªé”äº¤ç­†æ•¸", "æœªé”äº¤æ¯”ä¾‹(%)"], ascending=[False, False])
        st.write("**ä¾å®¢æˆ¶åç¨±çµ±è¨ˆï¼šæœªé”äº¤ç­†æ•¸èˆ‡æ¯”ä¾‹ï¼ˆ>0ï¼‰**")
        st.dataframe(customer_perf, use_container_width=True)
        st.download_button(
            "ä¸‹è¼‰æœªé”äº¤çµ±è¨ˆï¼ˆå®¢æˆ¶ï¼‰ CSV",
            data=customer_perf.to_csv(index=False).encode("utf-8-sig"),
            file_name="æœªé”äº¤çµ±è¨ˆ_ä¾å®¢æˆ¶.csv",
            mime="text/csv",
        )

def display_area_analysis(df: pd.DataFrame, col_map: Dict[str, str | None], exclude_swi_mask: pd.Series, topn_cities: int):
    """â‘¢ é¡¯ç¤ºé…é€å€åŸŸåˆ†æã€‚"""
    st.subheader("â‘¢ é…é€å€åŸŸåˆ†æï¼ˆæ’é™¤ SWI-å¯„åº«ï¼‰")
    address_col = col_map.get("address")
    cust_name_col = col_map.get("cust_name")

    if not address_col or address_col not in df.columns:
        st.warning("æ‰¾ä¸åˆ°ã€åœ°å€ã€æ¬„ä½ã€‚")
        return

    region_df = df[exclude_swi_mask].copy()
    region_df["ç¸£å¸‚"] = region_df[address_col].apply(extract_city).fillna("(æœªçŸ¥)")
    city_counts = region_df["ç¸£å¸‚"].value_counts(dropna=False).rename_axis("ç¸£å¸‚").reset_index(name="ç­†æ•¸")
    total_city = int(city_counts["ç­†æ•¸"].sum())
    city_counts["ç¸£å¸‚ä½”æ¯”(%)"] = (city_counts["ç­†æ•¸"] / total_city * 100).round(2) if total_city > 0 else 0.0

    c1, c2 = st.columns([1, 1])
    with c1:
        st.write("**å„ç¸£å¸‚é…é€ç­†æ•¸**")
        st.dataframe(city_counts, use_container_width=True)
        st.download_button(
            "ä¸‹è¼‰å„ç¸£å¸‚é…é€ç­†æ•¸ CSV",
            data=city_counts.to_csv(index=False).encode("utf-8-sig"),
            file_name="å„ç¸£å¸‚é…é€ç­†æ•¸.csv",
            mime="text/csv",
        )
    with c2:
        fig_city = px.bar(city_counts, x="ç¸£å¸‚", y="ç­†æ•¸")
        st.plotly_chart(fig_city, use_container_width=True)

    if cust_name_col and cust_name_col in region_df.columns:
        cust_city = region_df.groupby([cust_name_col, "ç¸£å¸‚"]).size().reset_index(name="ç­†æ•¸")
        cust_city["å®¢æˆ¶ç¸½ç­†æ•¸"] = cust_city.groupby(cust_name_col)["ç­†æ•¸"].transform("sum")
        cust_city["ç¸£å¸‚ä½”æ¯”(%)"] = (cust_city["ç­†æ•¸"] / cust_city["å®¢æˆ¶ç¸½ç­†æ•¸"] * 100).round(2)
        top_table = cust_city.sort_values("ç­†æ•¸", ascending=False).groupby(cust_name_col).head(topn_cities)
        st.write(f"**å„å®¢æˆ¶å¸¸å‡ºè²¨ç¸£å¸‚ï¼ˆæ¯å®¢æˆ¶ Top {topn_cities}ï¼‰**")
        st.dataframe(top_table.rename(columns={cust_name_col: "å®¢æˆ¶åç¨±"}), use_container_width=True)
        st.download_button(
            "ä¸‹è¼‰å„å®¢æˆ¶å¸¸å‡ºè²¨ç¸£å¸‚ CSV",
            data=top_table.to_csv(index=False).encode("utf-8-sig"),
            file_name="å„å®¢æˆ¶å¸¸å‡ºè²¨ç¸£å¸‚_TopN.csv",
            mime="text/csv",
        )

def display_loading_analysis(df: pd.DataFrame, col_map: Dict[str, str | None], exclude_swi_mask: pd.Series):
    """â‘£ é¡¯ç¤ºé…é€è£è¼‰åˆ†æã€‚"""
    st.subheader("â‘£ é…é€è£è¼‰åˆ†æï¼ˆå‡ºåº«å–®è™Ÿå‰13ç¢¼=åŒè»Šï¼›åƒ…å®Œæˆï¼‰")
    ship_no_col = col_map.get("ship_no")
    status_col = col_map.get("status")
    
    if not ship_no_col or ship_no_col not in df.columns:
        st.warning("æ‰¾ä¸åˆ°ã€å‡ºåº«å–®è™Ÿã€æ¬„ä½ã€‚")
        return

    base_mask = exclude_swi_mask
    if status_col and status_col in df.columns:
        base_mask &= (df[status_col].astype(str).str.strip() == "å®Œæˆ")
    else:
        st.info("æœªå°æ‡‰åˆ°ã€é€šçŸ¥å–®é …æ¬¡ç‹€æ…‹ã€æ¬„ä½ï¼Œå°‡ä¸å¥—ç”¨ã€Œå®Œæˆã€éæ¿¾ã€‚")

    load_df = df[base_mask].copy()
    if load_df.empty:
        st.info("æ ¹æ“šç¯©é¸æ¢ä»¶ï¼Œç„¡ä»»ä½•ã€å®Œæˆã€çš„å‡ºåº«å–®è³‡æ–™ã€‚")
        return

    load_df["è»Šæ¬¡ä»£ç¢¼"] = load_df[ship_no_col].astype(str).str[:13]
    load_df["_qty_num"] = pd.to_numeric(load_df[col_map.get("qty")], errors="coerce")
    load_df["_copper_ton_calc"] = pd.to_numeric(load_df[col_map.get("copper_ton")], errors="coerce") / 1000.0
    load_df["_fg_net_ton"] = pd.to_numeric(load_df[col_map.get("fg_net_ton")], errors="coerce")
    load_df["_fg_gross_ton"] = pd.to_numeric(load_df[col_map.get("fg_gross_ton")], errors="coerce")

    display_cols = {
        "è»Šæ¬¡ä»£ç¢¼": load_df["è»Šæ¬¡ä»£ç¢¼"],
        "å‡ºåº«å–®è™Ÿ": load_df[ship_no_col],
        "DOè™Ÿ": load_df.get(col_map.get("do_no")),
        "æ–™è™Ÿèªªæ˜": load_df.get(col_map.get("item_desc")),
        "æ‰¹æ¬¡": load_df.get(col_map.get("lot_no")),
        "å‡ºè²¨æ•¸é‡": load_df["_qty_num"],
        "éŠ…é‡é‡(å™¸)": load_df["_copper_ton_calc"],
        "æˆå“æ·¨é‡(å™¸)": load_df["_fg_net_ton"],
        "æˆå“æ¯›é‡(å™¸)": load_df["_fg_gross_ton"],
    }
    
    display_df = pd.DataFrame({k: v for k, v in display_cols.items() if v is not None})
    for col in ["å‡ºè²¨æ•¸é‡", "éŠ…é‡é‡(å™¸)", "æˆå“æ·¨é‡(å™¸)", "æˆå“æ¯›é‡(å™¸)"]:
        if col in display_df.columns:
            display_df[col] = pd.to_numeric(display_df[col], errors="coerce").round(3)
    
    sort_keys = [c for c in ["è»Šæ¬¡ä»£ç¢¼", "å‡ºåº«å–®è™Ÿ"] if c in display_df.columns]
    if sort_keys:
        display_df = display_df.sort_values(by=sort_keys).reset_index(drop=True)

    st.write("**é…é€è£è¼‰æ¸…å–®**")
    st.dataframe(display_df, use_container_width=True)
    st.download_button(
        "ä¸‹è¼‰é…é€è£è¼‰æ¸…å–® CSV",
        data=display_df.to_csv(index=False).encode("utf-8-sig"),
        file_name="é…é€è£è¼‰æ¸…å–®.csv",
        mime="text/csv",
    )

    summary = load_df.groupby("è»Šæ¬¡ä»£ç¢¼").agg(
        å‡ºè²¨æ•¸é‡å°è¨ˆ=("_qty_num", "sum"),
        éŠ…é‡é‡å™¸å°è¨ˆ=("_copper_ton_calc", "sum"),
        æˆå“æ·¨é‡å™¸å°è¨ˆ=("_fg_net_ton", "sum"),
        æˆå“æ¯›é‡å™¸å°è¨ˆ=("_fg_gross_ton", "sum")
    ).round(3).reset_index()

    st.write("**è»Šæ¬¡å½™ç¸½ï¼ˆæ¯è»Šå°è¨ˆï¼‰**")
    st.dataframe(summary, use_container_width=True)
    st.download_button(
        "ä¸‹è¼‰è»Šæ¬¡å½™ç¸½ CSV",
        data=summary.to_csv(index=False).encode("utf-8-sig"),
        file_name="è»Šæ¬¡å½™ç¸½.csv",
        mime="text/csv",
    )

    if "éŠ…é‡é‡å™¸å°è¨ˆ" in summary.columns:
        valid_trips = summary[summary["éŠ…é‡é‡å™¸å°è¨ˆ"] > 0]
        avg_load = valid_trips["éŠ…é‡é‡å™¸å°è¨ˆ"].mean() if not valid_trips.empty else 0.0
        k1, k2 = st.columns(2)
        k1.metric("ç´å…¥è¨ˆç®—è»Šæ¬¡", f"{len(valid_trips):,}")
        k2.metric("å¹³å‡è»Šè¼›è¼‰é‡(å™¸)", f"{avg_load:.2f}")

# ========================
# 3. Streamlit ä¸»æµç¨‹
# ========================

file = st.file_uploader(
    "ä¸Šå‚³ Excel æˆ– CSV æª”",
    type=["xlsx", "xls", "csv"],
    help="æœ€å¤š 200 MBï¼›Excel éœ€ä½¿ç”¨ openpyxl è§£æ"
)

if file:
    file_ext = file.name.split(".")[-1].lower()
    df = load_data(file, file_ext)

    if not df.empty:
        col_map = guess_column_names(df.columns)
        
        tab_analysis, tab_raw = st.tabs(["ğŸ“Š å‡ºè²¨&é”äº¤åˆ†æ", "ğŸ“„ åŸå§‹è³‡æ–™é è¦½"])

        with tab_raw:
            st.subheader("åŸå§‹è³‡æ–™é è¦½")
            st.dataframe(df, use_container_width=True)

        with tab_analysis:
            with st.sidebar:
                st.header("âš™ï¸ ç¯©é¸æ“ä½œå€")
                st.info("æ‚¨å¯ä»¥é¸æ“‡å¤šå€‹æ¢ä»¶ä¾†äº¤å‰åˆ†ææ•¸æ“šã€‚")
                
                # ä½¿ç”¨ st.expander å°‡ä¸å¸¸ç”¨ç¯©é¸æ”¶åˆ
                with st.expander("å¸¸ç”¨ç¯©é¸", expanded=True):
                    # å¸¸ç”¨ç¯©é¸
                    sel_types = st.multiselect("å‡ºè²¨ç”³è«‹é¡å‹", options=sorted(df[col_map["ship_type"]].dropna().unique()) if col_map["ship_type"] else [], default=[])
                    sel_names = st.multiselect("å®¢æˆ¶åç¨±", options=sorted(df[col_map["cust_name"]].dropna().unique()) if col_map["cust_name"] else [], default=[])
                    date_range = None
                    if col_map["due_date"]:
                        due_series = to_dt(df[col_map["due_date"]])
                        min_date, max_date = due_series.min().date(), due_series.max().date()
                        if min_date and max_date:
                            date_range = st.date_input("æŒ‡å®šåˆ°è²¨æ—¥æœŸï¼ˆå€é–“ï¼‰", value=(min_date, max_date))
                    
                    st.slider_label = "æ¯å®¢æˆ¶ Top N ç¸£å¸‚"
                    topn_cities = st.slider(st.slider_label, min_value=1, max_value=10, value=3, step=1)
                
                with st.expander("é€²éšç¯©é¸", expanded=False):
                    sel_ids = st.multiselect("å®¢æˆ¶ç·¨è™Ÿ", options=sorted(df[col_map["cust_id"]].dropna().unique()) if col_map["cust_id"] else [], default=[])
                    sel_items = st.multiselect("æ–™è™Ÿèªªæ˜", options=sorted(df[col_map["item_desc"]].dropna().unique()) if col_map["item_desc"] else [], default=[])
                    filter_col = st.selectbox("é¸æ“‡é¡å¤–ç¯©é¸æ¬„ä½", ["ï¼ˆä¸ç¯©é¸ï¼‰"] + list(df.columns))
                    extra_selected_vals = []
                    if filter_col != "ï¼ˆä¸ç¯©é¸ï¼‰":
                        extra_selected_vals = st.multiselect("é¸å–å€¼", df[filter_col].dropna().unique().tolist())
                
            filtered_df = df.copy()
            if sel_types:
                filtered_df = filtered_df[filtered_df[col_map["ship_type"]].isin(sel_types)]
            if sel_names:
                filtered_df = filtered_df[filtered_df[col_map["cust_name"]].isin(sel_names)]
            if sel_ids:
                filtered_df = filtered_df[filtered_df[col_map["cust_id"]].isin(sel_ids)]
            if sel_items:
                filtered_df = filtered_df[filtered_df[col_map["item_desc"]].isin(sel_items)]
            if date_range and len(date_range) == 2:
                start_d, end_d = pd.to_datetime(date_range[0]), pd.to_datetime(date_range[1])
                due_day = to_dt(filtered_df[col_map["due_date"]]).dt.normalize()
                filtered_df = filtered_df[(due_day >= start_d) & (due_day <= end_d)]
            if filter_col != "ï¼ˆä¸ç¯©é¸ï¼‰" and extra_selected_vals:
                filtered_df = filtered_df[filtered_df[filter_col].isin(extra_selected_vals)]
            
            # å…±ç”¨ï¼šæ’é™¤ SWI-å¯„åº«
            exclude_swi_mask = (filtered_df[col_map["ship_type"]] != "SWI-å¯„åº«") if col_map.get("ship_type") else pd.Series(True, index=filtered_df.index)
            
            # é¡¯ç¤ºå„åˆ†æå€å¡Š
            display_shipment_type_analysis(filtered_df.copy(), col_map)
            st.markdown("---")
            display_delivery_performance(filtered_df.copy(), col_map, exclude_swi_mask)
            st.markdown("---")
            display_area_analysis(filtered_df.copy(), col_map, exclude_swi_mask, topn_cities)
            st.markdown("---")
            display_loading_analysis(filtered_df.copy(), col_map, exclude_swi_mask)
            
    else:
        st.warning("æª”æ¡ˆè¼‰å…¥å¤±æ•—æˆ–ç‚ºç©ºã€‚")
else:
    st.info("è«‹å…ˆåœ¨ä¸Šæ–¹ä¸Šå‚³ Excel æˆ– CSV æª”ä»¥é–‹å§‹åˆ†æã€‚")
