import io
import re
from typing import Dict, Optional, Tuple
import pandas as pd
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go

# ========================
# é é¢è¨­å®šèˆ‡ CSS æ¨£å¼
# ========================

st.set_page_config(
    page_title="TMSå‡ºè²¨é…é€æ•¸æ“šåˆ†æ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾© CSS æ¨£å¼
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #4CAF50, #2196F3);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        font-size: 2.5rem;
        font-weight: bold;
        margin-bottom: 1rem;
    }
    
    .metric-container {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border-left: 4px solid #4CAF50;
        margin: 0.5rem 0;
    }
    
    .section-header {
        color: #2c3e50;
        border-bottom: 2px solid #3498db;
        padding-bottom: 0.5rem;
        margin: 1.5rem 0 1rem 0;
    }
    
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    
    .success-box {
        background-color: #d1edff;
        border: 1px solid #74b9ff;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ä¸»æ¨™é¡Œ
st.markdown('<h1 class="main-header">ğŸ“Š TMSå‡ºè²¨é…é€æ•¸æ“šåˆ†æ</h1>', unsafe_allow_html=True)
st.markdown("""
<div style="text-align: center; color: #7f8c8d; margin-bottom: 2rem;">
    <strong>æ™ºèƒ½åŒ–æ•¸æ“šåˆ†æå¹³å°</strong> | ä¸Šå‚³ Excel/CSV â†’ å‡ºè²¨é¡å‹åˆ†æ â†’ é…é€é”äº¤ç‡ â†’ å€åŸŸåˆ†æ â†’ è£è¼‰åˆ†æ
</div>
""", unsafe_allow_html=True)

# ========================
# å·¥å…·å‡½å¼å„ªåŒ–
# ========================

@st.cache_data(show_spinner="æ­£åœ¨è¼‰å…¥æª”æ¡ˆ...")
def load_data(file_buffer: io.BytesIO, file_type: str) -> pd.DataFrame:
    """
    é«˜æ•ˆèƒ½æª”æ¡ˆè¼‰å…¥èˆ‡å¿«å–
    
    Args:
        file_buffer: æª”æ¡ˆç·©è¡å€
        file_type: æª”æ¡ˆé¡å‹ (csv, xlsx, xls)
    
    Returns:
        DataFrame: è¼‰å…¥çš„æ•¸æ“š
    """
    try:
        if file_type == "csv":
            # è‡ªå‹•åµæ¸¬ç·¨ç¢¼
            encodings = ['utf-8', 'utf-8-sig', 'big5', 'gbk']
            for encoding in encodings:
                try:
                    file_buffer.seek(0)
                    df = pd.read_csv(file_buffer, encoding=encoding)
                    return df
                except UnicodeDecodeError:
                    continue
            # å¦‚æœéƒ½å¤±æ•—ï¼Œä½¿ç”¨é è¨­ç·¨ç¢¼
            file_buffer.seek(0)
            return pd.read_csv(file_buffer, encoding='utf-8', errors='ignore')
        elif file_type in ["xlsx", "xls"]:
            return pd.read_excel(file_buffer, engine="openpyxl")
    except Exception as e:
        st.error(f"æª”æ¡ˆè¼‰å…¥å¤±æ•—: {str(e)}")
        return pd.DataFrame()
    
    st.error("ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹ã€‚è«‹ä¸Šå‚³ .xlsx, .xls æˆ– .csv æª”æ¡ˆã€‚")
    return pd.DataFrame()

def smart_column_detector(df_cols: list) -> Dict[str, Optional[str]]:
    """
    æ™ºèƒ½æ¬„ä½åµæ¸¬å™¨ - æå‡åµæ¸¬æº–ç¢ºåº¦
    
    Args:
        df_cols: DataFrame æ¬„ä½åˆ—è¡¨
    
    Returns:
        Dict: åµæ¸¬åˆ°çš„æ¬„ä½å°æ‡‰å­—å…¸
    """
    def find_best_match(keywords: list, priority_keywords: list = None) -> Optional[str]:
        """å°‹æ‰¾æœ€ä½³åŒ¹é…æ¬„ä½ï¼Œæ”¯æ´å„ªå…ˆç´šæœå°‹"""
        # å„ªå…ˆæœå°‹é«˜å„ªå…ˆç´šé—œéµå­—
        if priority_keywords:
            for kw in priority_keywords:
                for col in df_cols:
                    if kw.lower() in str(col).lower():
                        return col
        
        # ä¸€èˆ¬æœå°‹
        for kw in keywords:
            for col in df_cols:
                if kw.lower() in str(col).lower():
                    return col
        return None

    column_mapping = {
        "ship_type": find_best_match(
            ["å‡ºè²¨ç”³è«‹é¡å‹", "å‡ºè²¨é¡å‹", "é…é€é¡å‹", "é¡å‹", "ç”³è«‹é¡å‹"],
            ["å‡ºè²¨ç”³è«‹é¡å‹", "å‡ºè²¨é¡å‹"]
        ),
        "due_date": find_best_match(
            ["æŒ‡å®šåˆ°è²¨æ—¥æœŸ", "åˆ°è²¨æ—¥æœŸ", "æŒ‡å®šåˆ°è²¨", "åˆ°è²¨æ—¥", "é è¨ˆåˆ°è²¨"],
            ["æŒ‡å®šåˆ°è²¨æ—¥æœŸ"]
        ),
        "sign_date": find_best_match(
            ["å®¢æˆ¶ç°½æ”¶æ—¥æœŸ", "ç°½æ”¶æ—¥æœŸ", "ç°½æ”¶æ—¥", "ç°½æ”¶æ™‚é–“"],
            ["å®¢æˆ¶ç°½æ”¶æ—¥æœŸ"]
        ),
        "cust_id": find_best_match(
            ["å®¢æˆ¶ç·¨è™Ÿ", "å®¢æˆ¶ä»£è™Ÿ", "å®¢ç·¨", "å®¢æˆ¶id"],
            ["å®¢æˆ¶ç·¨è™Ÿ"]
        ),
        "cust_name": find_best_match(
            ["å®¢æˆ¶åç¨±", "å®¢å", "å®¢æˆ¶", "å®¢æˆ¶ç°¡ç¨±"],
            ["å®¢æˆ¶åç¨±"]
        ),
        "address": find_best_match(
            ["åœ°å€", "æ”¶è²¨åœ°å€", "é€è²¨åœ°å€", "äº¤è²¨åœ°å€", "é…é€åœ°å€"],
            ["åœ°å€"]
        ),
        "copper_ton": find_best_match(
            ["éŠ…é‡é‡(å™¸)", "éŠ…é‡é‡(å™¸æ•¸)", "éŠ…å™¸", "éŠ…é‡é‡", "éŠ…é‡é‡ï¼ˆå™¸ï¼‰"],
            ["éŠ…é‡é‡(å™¸)", "éŠ…é‡é‡ï¼ˆå™¸ï¼‰"]
        ),
        "qty": find_best_match(
            ["å‡ºè²¨æ•¸é‡", "æ•¸é‡", "å‡ºè²¨é‡", "å‡ºåº«æ•¸é‡"],
            ["å‡ºè²¨æ•¸é‡"]
        ),
        "ship_no": find_best_match(
            ["å‡ºåº«å–®è™Ÿ", "å‡ºåº«å–®", "å‡ºåº«ç·¨è™Ÿ", "å‡ºè²¨å–®è™Ÿ"],
            ["å‡ºåº«å–®è™Ÿ"]
        ),
        "do_no": find_best_match(
            ["DOè™Ÿ", "DO", "å‡ºè²¨å–®è™Ÿ", "äº¤è²¨å–®è™Ÿ", "doè™Ÿ"],
            ["DOè™Ÿ"]
        ),
        "item_desc": find_best_match(
            ["æ–™è™Ÿèªªæ˜", "å“å", "å“åè¦æ ¼", "ç‰©æ–™èªªæ˜", "ç”¢å“åç¨±"],
            ["æ–™è™Ÿèªªæ˜", "å“å"]
        ),
        "lot_no": find_best_match(
            ["æ‰¹æ¬¡", "æ‰¹è™Ÿ", "Lot", "lot", "æ‰¹æ¬¡è™Ÿ"],
            ["æ‰¹æ¬¡", "æ‰¹è™Ÿ"]
        ),
        "fg_net_ton": find_best_match(
            ["æˆå“æ·¨é‡(å™¸)", "æˆå“æ·¨é‡(å™¸)", "æ·¨é‡(å™¸)", "æ·¨é‡"],
            ["æˆå“æ·¨é‡(å™¸)"]
        ),
        "fg_gross_ton": find_best_match(
            ["æˆå“æ¯›é‡(å™¸)", "æˆå“æ¯›é‡(å™¸)", "æ¯›é‡(å™¸)", "æ¯›é‡"],
            ["æˆå“æ¯›é‡(å™¸)"]
        ),
        "status": find_best_match(
            ["é€šçŸ¥å–®é …æ¬¡ç‹€æ…‹", "é …æ¬¡ç‹€æ…‹", "ç‹€æ…‹", "å–®æ“šç‹€æ…‹"],
            ["é€šçŸ¥å–®é …æ¬¡ç‹€æ…‹"]
        ),
    }
    
    return column_mapping

def safe_datetime_convert(series: pd.Series) -> pd.Series:
    """å®‰å…¨çš„æ—¥æœŸæ™‚é–“è½‰æ›ï¼Œæ”¯æ´å¤šç¨®æ ¼å¼"""
    if series.empty:
        return series
    
    # å˜—è©¦å¤šç¨®æ—¥æœŸæ ¼å¼
    formats = [
        "%Y-%m-%d",
        "%Y/%m/%d", 
        "%Y-%m-%d %H:%M:%S",
        "%Y/%m/%d %H:%M:%S",
        "%m/%d/%Y",
        "%d/%m/%Y"
    ]
    
    result = pd.to_datetime(series, errors="coerce")
    
    # å¦‚æœè½‰æ›å¤±æ•—ç‡å¤ªé«˜ï¼Œå˜—è©¦å…¶ä»–æ ¼å¼
    if result.isna().sum() / len(result) > 0.5:
        for fmt in formats:
            try:
                temp_result = pd.to_datetime(series, format=fmt, errors="coerce")
                if temp_result.isna().sum() < result.isna().sum():
                    result = temp_result
                    break
            except:
                continue
    
    return result

def enhanced_city_extraction(address: str) -> Optional[str]:
    """
    å¢å¼·ç‰ˆå°ç£ç¸£å¸‚æ“·å–ï¼Œæ”¯æ´æ›´å¤šæ ¼å¼
    
    Args:
        address: åœ°å€å­—ä¸²
        
    Returns:
        Optional[str]: æ“·å–åˆ°çš„ç¸£å¸‚åç¨±
    """
    if pd.isna(address) or not address:
        return None
    
    # æ­£è¦åŒ–åœ°å€
    normalized_addr = str(address).strip().replace("è‡º", "å°").replace("ã€€", " ")
    
    # å°ç£ç¸£å¸‚åˆ—è¡¨ï¼ˆåŒ…å«å¸¸è¦‹è®Šé«”ï¼‰
    taiwan_cities = [
        "å°åŒ—å¸‚", "æ–°åŒ—å¸‚", "æ¡ƒåœ’å¸‚", "å°ä¸­å¸‚", "å°å—å¸‚", "é«˜é›„å¸‚",
        "åŸºéš†å¸‚", "æ–°ç«¹å¸‚", "æ–°ç«¹ç¸£", "è‹—æ —ç¸£", "å½°åŒ–ç¸£", "å—æŠ•ç¸£",
        "é›²æ—ç¸£", "å˜‰ç¾©å¸‚", "å˜‰ç¾©ç¸£", "å±æ±ç¸£", "å®œè˜­ç¸£", "èŠ±è“®ç¸£",
        "å°æ±ç¸£", "æ¾æ¹–ç¸£", "é‡‘é–€ç¸£", "é€£æ±Ÿç¸£"
    ]
    
    # å„ªå…ˆå®Œæ•´åŒ¹é…
    for city in taiwan_cities:
        if city in normalized_addr:
            return city
    
    # æ¬¡è¦ï¼šæ¨¡ç³ŠåŒ¹é…ï¼ˆå»æ‰å¸‚/ç¸£ï¼‰
    for city in taiwan_cities:
        city_base = city.replace("å¸‚", "").replace("ç¸£", "")
        if city_base in normalized_addr and len(city_base) >= 2:
            return city
            
    return None

def create_enhanced_metric_card(title: str, value: str, delta: str = None, delta_color: str = "normal"):
    """å»ºç«‹å¢å¼·ç‰ˆæŒ‡æ¨™å¡ç‰‡"""
    delta_html = ""
    if delta:
        color = {"normal": "#666", "positive": "#28a745", "negative": "#dc3545"}.get(delta_color, "#666")
        delta_html = f'<div style="color: {color}; font-size: 0.9rem; margin-top: 0.5rem;">{delta}</div>'
    
    st.markdown(f"""
    <div class="metric-container">
        <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">{title}</div>
        <div style="font-size: 2rem; font-weight: bold; color: #2c3e50;">{value}</div>
        {delta_html}
    </div>
    """, unsafe_allow_html=True)

# ========================
# æ ¸å¿ƒåˆ†æå‡½å¼å„ªåŒ–
# ========================

def enhanced_shipment_analysis(df: pd.DataFrame, col_map: Dict[str, Optional[str]]) -> None:
    """
    å¢å¼·ç‰ˆå‡ºè²¨é¡å‹åˆ†æ
    
    Args:
        df: è³‡æ–™æ¡†
        col_map: æ¬„ä½å°æ‡‰å­—å…¸
    """
    st.markdown('<h2 class="section-header">â‘  å‡ºè²¨é¡å‹ç­†æ•¸èˆ‡é‡é‡çµ±è¨ˆ</h2>', unsafe_allow_html=True)
    
    ship_type_col = col_map.get("ship_type")
    copper_ton_col = col_map.get("copper_ton")
    
    if not ship_type_col or ship_type_col not in df.columns:
        st.error("âš ï¸ æ‰¾ä¸åˆ°ã€å‡ºè²¨ç”³è«‹é¡å‹ã€æ¬„ä½ï¼Œè«‹ç¢ºèªæª”æ¡ˆæ ¼å¼æ˜¯å¦æ­£ç¢ºã€‚")
        return
    
    # è™•ç†å‡ºè²¨é¡å‹çµ±è¨ˆ
    df_work = df.copy()
    df_work[ship_type_col] = df_work[ship_type_col].fillna("(ç©ºç™½)")
    
    type_stats = df_work[ship_type_col].value_counts().reset_index()
    type_stats.columns = ["å‡ºè²¨é¡å‹", "ç­†æ•¸"]
    
    # è¨ˆç®—éŠ…é‡é‡çµ±è¨ˆ
    if copper_ton_col and copper_ton_col in df.columns:
        df_work["_copper_numeric"] = pd.to_numeric(df_work[copper_ton_col], errors="coerce")
        copper_stats = df_work.groupby(ship_type_col)["_copper_numeric"].agg([
            ('éŠ…é‡é‡_kg_åˆè¨ˆ', 'sum'),
            ('å¹³å‡éŠ…é‡é‡_kg', 'mean')
        ]).reset_index()
        copper_stats.columns = ["å‡ºè²¨é¡å‹", "éŠ…é‡é‡(kg)åˆè¨ˆ", "å¹³å‡éŠ…é‡é‡(kg)"]
        copper_stats["éŠ…é‡é‡(å™¸)åˆè¨ˆ"] = (copper_stats["éŠ…é‡é‡(kg)åˆè¨ˆ"] / 1000).round(3)
        copper_stats["å¹³å‡éŠ…é‡é‡(å™¸)"] = (copper_stats["å¹³å‡éŠ…é‡é‡(kg)"] / 1000).round(3)
        
        # åˆä½µçµ±è¨ˆæ•¸æ“š
        final_stats = type_stats.merge(
            copper_stats[["å‡ºè²¨é¡å‹", "éŠ…é‡é‡(å™¸)åˆè¨ˆ", "å¹³å‡éŠ…é‡é‡(å™¸)"]], 
            on="å‡ºè²¨é¡å‹", 
            how="left"
        )
    else:
        final_stats = type_stats.copy()
        final_stats["éŠ…é‡é‡(å™¸)åˆè¨ˆ"] = None
        final_stats["å¹³å‡éŠ…é‡é‡(å™¸)"] = None
    
    # è¨ˆç®—ç™¾åˆ†æ¯”
    total_records = final_stats["ç­†æ•¸"].sum()
    final_stats["ä½”æ¯”(%)"] = (final_stats["ç­†æ•¸"] / total_records * 100).round(2)
    
    # é¡¯ç¤ºé—œéµæŒ‡æ¨™
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        create_enhanced_metric_card("ç¸½è³‡æ–™ç­†æ•¸", f"{total_records:,}")
    with col2:
        unique_types = len(final_stats)
        create_enhanced_metric_card("å‡ºè²¨é¡å‹æ•¸", f"{unique_types}")
    with col3:
        total_copper = final_stats["éŠ…é‡é‡(å™¸)åˆè¨ˆ"].sum() if "éŠ…é‡é‡(å™¸)åˆè¨ˆ" in final_stats.columns else 0
        create_enhanced_metric_card("ç¸½éŠ…é‡é‡(å™¸)", f"{total_copper:,.3f}")
    with col4:
        avg_per_record = total_copper / total_records if total_records > 0 else 0
        create_enhanced_metric_card("å¹³å‡æ¯ç­†é‡é‡(å™¸)", f"{avg_per_record:.3f}")
    
    # é¡¯ç¤ºè©³ç´°çµ±è¨ˆè¡¨æ ¼èˆ‡åœ–è¡¨
    tab1, tab2 = st.tabs(["ğŸ“‹ è©³ç´°çµ±è¨ˆ", "ğŸ“Š è¦–è¦ºåŒ–åœ–è¡¨"])
    
    with tab1:
        st.dataframe(
            final_stats.style.format({
                "ç­†æ•¸": "{:,}",
                "ä½”æ¯”(%)": "{:.2f}%",
                "éŠ…é‡é‡(å™¸)åˆè¨ˆ": "{:.3f}",
                "å¹³å‡éŠ…é‡é‡(å™¸)": "{:.3f}"
            }),
            use_container_width=True,
            height=400
        )
        
        # ä¸‹è¼‰æŒ‰éˆ•
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰çµ±è¨ˆè¡¨ (CSV)",
                data=final_stats.to_csv(index=False).encode("utf-8-sig"),
                file_name="å‡ºè²¨é¡å‹çµ±è¨ˆ.csv",
                mime="text/csv",
            )
        with col2:
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰çµ±è¨ˆè¡¨ (Excel)",
                data=final_stats.to_excel(index=False).encode() if hasattr(final_stats.to_excel(index=False), 'encode') else final_stats.to_excel(index=False, engine='openpyxl'),
                file_name="å‡ºè²¨é¡å‹çµ±è¨ˆ.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
    
    with tab2:
        chart_col1, chart_col2 = st.columns(2)
        
        with chart_col1:
            chart_type = st.selectbox(
                "åœ–è¡¨é¡å‹", 
                ["é•·æ¢åœ–", "åœ“é¤…åœ–", "ç’°å½¢åœ–", "æ°´å¹³é•·æ¢åœ–"], 
                key="chart_type_1"
            )
            
            # å‰µå»ºäº’å‹•å¼åœ–è¡¨
            if chart_type == "é•·æ¢åœ–":
                fig = px.bar(
                    final_stats, 
                    x="å‡ºè²¨é¡å‹", 
                    y="ç­†æ•¸",
                    title="å‡ºè²¨é¡å‹ç­†æ•¸åˆ†ä½ˆ",
                    color="ç­†æ•¸",
                    color_continuous_scale="viridis"
                )
                fig.update_layout(xaxis_tickangle=-45)
            elif chart_type == "æ°´å¹³é•·æ¢åœ–":
                fig = px.bar(
                    final_stats, 
                    x="ç­†æ•¸", 
                    y="å‡ºè²¨é¡å‹",
                    orientation='h',
                    title="å‡ºè²¨é¡å‹ç­†æ•¸åˆ†ä½ˆ",
                    color="ç­†æ•¸",
                    color_continuous_scale="viridis"
                )
            elif chart_type == "åœ“é¤…åœ–":
                fig = px.pie(
                    final_stats, 
                    names="å‡ºè²¨é¡å‹", 
                    values="ç­†æ•¸",
                    title="å‡ºè²¨é¡å‹ä½”æ¯”åˆ†ä½ˆ"
                )
            else:  # ç’°å½¢åœ–
                fig = px.pie(
                    final_stats, 
                    names="å‡ºè²¨é¡å‹", 
                    values="ç­†æ•¸",
                    title="å‡ºè²¨é¡å‹ä½”æ¯”åˆ†ä½ˆ",
                    hole=0.4
                )
            
            fig.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig, use_container_width=True)
        
        with chart_col2:
            if "éŠ…é‡é‡(å™¸)åˆè¨ˆ" in final_stats.columns and final_stats["éŠ…é‡é‡(å™¸)åˆè¨ˆ"].notna().any():
                fig_copper = px.treemap(
                    final_stats[final_stats["éŠ…é‡é‡(å™¸)åˆè¨ˆ"].notna()],
                    path=["å‡ºè²¨é¡å‹"],
                    values="éŠ…é‡é‡(å™¸)åˆè¨ˆ",
                    title="éŠ…é‡é‡åˆ†ä½ˆæ¨¹ç‹€åœ–",
                    color="éŠ…é‡é‡(å™¸)åˆè¨ˆ",
                    color_continuous_scale="RdYlBu"
                )
                st.plotly_chart(fig_copper, use_container_width=True)
            else:
                st.info("ç„¡éŠ…é‡é‡æ•¸æ“šå¯ä¾›åˆ†æ")

def enhanced_delivery_performance(df: pd.DataFrame, col_map: Dict[str, Optional[str]], exclude_swi_mask: pd.Series) -> None:
    """
    å¢å¼·ç‰ˆé…é€é”äº¤ç‡åˆ†æ
    
    Args:
        df: è³‡æ–™æ¡†
        col_map: æ¬„ä½å°æ‡‰å­—å…¸
        exclude_swi_mask: æ’é™¤SWIé®ç½©
    """
    st.markdown('<h2 class="section-header">â‘¡ é…é€é”äº¤ç‡åˆ†æ</h2>', unsafe_allow_html=True)
    
    due_date_col = col_map.get("due_date")
    sign_date_col = col_map.get("sign_date")
    cust_id_col = col_map.get("cust_id")
    cust_name_col = col_map.get("cust_name")
    
    if not all([due_date_col, sign_date_col]) or not all([col in df.columns for col in [due_date_col, sign_date_col]]):
        st.error("âš ï¸ æ‰¾ä¸åˆ°å¿…è¦çš„æ—¥æœŸæ¬„ä½ï¼ˆæŒ‡å®šåˆ°è²¨æ—¥æœŸã€å®¢æˆ¶ç°½æ”¶æ—¥æœŸï¼‰")
        return
    
    # æ—¥æœŸè™•ç†
    due_day = safe_datetime_convert(df[due_date_col]).dt.normalize()
    sign_day = safe_datetime_convert(df[sign_date_col]).dt.normalize()
    
    # æœ‰æ•ˆæ€§æª¢æŸ¥
    valid_mask = due_day.notna() & sign_day.notna() & exclude_swi_mask
    on_time_mask = (sign_day <= due_day) & valid_mask
    late_mask = valid_mask & (sign_day > due_day)
    
    # çµ±è¨ˆè¨ˆç®—
    total_valid = int(valid_mask.sum())
    on_time_count = int(on_time_mask.sum())
    late_count = int(late_mask.sum())
    
    if total_valid == 0:
        st.warning("ç„¡æœ‰æ•ˆçš„é”äº¤åˆ†ææ•¸æ“š")
        return
    
    delivery_rate = on_time_count / total_valid * 100
    delay_days = (sign_day - due_day).dt.days
    avg_delay = delay_days[late_mask].mean() if late_count > 0 else 0.0
    max_delay = delay_days[late_mask].max() if late_count > 0 else 0
    
    # é—œéµæŒ‡æ¨™å±•ç¤º
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        create_enhanced_metric_card("æœ‰æ•ˆç­†æ•¸", f"{total_valid:,}")
    with col2:
        create_enhanced_metric_card("æº–æ™‚äº¤ä»˜", f"{on_time_count:,}", f"({delivery_rate:.1f}%)", "positive")
    with col3:
        create_enhanced_metric_card("å»¶é²äº¤ä»˜", f"{late_count:,}", f"({100-delivery_rate:.1f}%)", "negative")
    with col4:
        create_enhanced_metric_card("é”äº¤ç‡", f"{delivery_rate:.2f}%", 
                                   "å„ªç•°" if delivery_rate >= 95 else "è‰¯å¥½" if delivery_rate >= 90 else "éœ€æ”¹å–„",
                                   "positive" if delivery_rate >= 95 else "normal" if delivery_rate >= 90 else "negative")
    with col5:
        create_enhanced_metric_card("å¹³å‡å»¶é²", f"{avg_delay:.1f}å¤©", f"æœ€å¤§: {max_delay}å¤©" if max_delay > 0 else "")
    
    # è©³ç´°åˆ†æ
    analysis_tab1, analysis_tab2, analysis_tab3 = st.tabs(["ğŸšš æœªé”æ¨™æ˜ç´°", "ğŸ“ˆ é”äº¤è¶¨å‹¢", "ğŸ‘¥ å®¢æˆ¶è¡¨ç¾"])
    
    with analysis_tab1:
        if late_count > 0:
            late_details = pd.DataFrame({
                col: df[col] if col else None for col in [cust_id_col, cust_name_col]
            }).assign(
                æŒ‡å®šåˆ°è²¨æ—¥æœŸ=due_day.dt.strftime("%Y-%m-%d"),
                å®¢æˆ¶ç°½æ”¶æ—¥æœŸ=sign_day.dt.strftime("%Y-%m-%d"),
                å»¶é²å¤©æ•¸=delay_days
            )[late_mask].dropna(axis=1, how="all")
            
            # å»¶é²ç¨‹åº¦åˆ†é¡
            late_details["å»¶é²ç¨‹åº¦"] = pd.cut(
                late_details["å»¶é²å¤©æ•¸"],
                bins=[-float('inf'), 0, 3, 7, 30, float('inf')],
                labels=["æº–æ™‚", "è¼•å¾®å»¶é²(1-3å¤©)", "ä¸­åº¦å»¶é²(4-7å¤©)", "é‡åº¦å»¶é²(8-30å¤©)", "åš´é‡å»¶é²(>30å¤©)"]
            )
            
            # æ’åºï¼šå»¶é²å¤©æ•¸é™åº
            late_details = late_details.sort_values("å»¶é²å¤©æ•¸", ascending=False)
            
            st.dataframe(
                late_details.style.format({"å»¶é²å¤©æ•¸": "{:.0f}å¤©"}),
                use_container_width=True,
                height=400
            )
            
            # å»¶é²åˆ†ä½ˆåˆ†æ
            delay_dist = late_details["å»¶é²ç¨‹åº¦"].value_counts()
            fig_delay = px.bar(
                x=delay_dist.index, 
                y=delay_dist.values,
                title="å»¶é²ç¨‹åº¦åˆ†ä½ˆ",
                labels={"x": "å»¶é²ç¨‹åº¦", "y": "ç­†æ•¸"}
            )
            st.plotly_chart(fig_delay, use_container_width=True)
            
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰æœªé”æ¨™æ˜ç´°",
                data=late_details.to_csv(index=False).encode("utf-8-sig"),
                file_name="æœªé”æ¨™æ˜ç´°.csv",
                mime="text/csv",
            )
        else:
            st.success("ğŸ‰ æ­å–œï¼æ‰€æœ‰å‡ºè²¨éƒ½æº–æ™‚é”äº¤ï¼")
    
    with analysis_tab2:
        if total_valid >= 7:  # è‡³å°‘è¦æœ‰ä¸€é€±çš„æ•¸æ“š
            # ä¾æ—¥æœŸçµ±è¨ˆé”äº¤ç‡è¶¨å‹¢
            df_trend = df[valid_mask].copy()
            df_trend["åˆ°è²¨æ—¥æœŸ"] = due_day[valid_mask]
            df_trend["æº–æ™‚"] = on_time_mask[valid_mask]
            
            daily_trend = df_trend.groupby(df_trend["åˆ°è²¨æ—¥æœŸ"].dt.date).agg(
                ç¸½ç­†æ•¸=("æº–æ™‚", "count"),
                æº–æ™‚ç­†æ•¸=("æº–æ™‚", "sum")
            )
            daily_trend["é”äº¤ç‡(%)"] = (daily_trend["æº–æ™‚ç­†æ•¸"] / daily_trend["ç¸½ç­†æ•¸"] * 100).round(2)
            daily_trend = daily_trend.reset_index()
            
            fig_trend = px.line(
                daily_trend, 
                x="åˆ°è²¨æ—¥æœŸ", 
                y="é”äº¤ç‡(%)",
                title="æ¯æ—¥é”äº¤ç‡è¶¨å‹¢",
                markers=True
            )
            fig_trend.add_hline(y=95, line_dash="dash", line_color="green", annotation_text="ç›®æ¨™ç·š(95%)")
            fig_trend.add_hline(y=90, line_dash="dash", line_color="orange", annotation_text="è­¦æˆ’ç·š(90%)")
            
            st.plotly_chart(fig_trend, use_container_width=True)
        else:
            st.info("æ•¸æ“šé‡ä¸è¶³ï¼Œç„¡æ³•åˆ†æè¶¨å‹¢ï¼ˆéœ€è‡³å°‘7ç­†è¨˜éŒ„ï¼‰")
    
    with analysis_tab3:
        if cust_name_col and cust_name_col in df.columns:
            customer_performance = df[valid_mask].groupby(cust_name_col).agg(
                æœ‰æ•ˆç­†æ•¸=("å®¢æˆ¶ç°½æ”¶æ—¥æœŸ", "count") if "å®¢æˆ¶ç°½æ”¶æ—¥æœŸ" in df.columns else (cust_name_col, "count"),
                æº–æ™‚ç­†æ•¸=(on_time_mask[valid_mask], "sum"),
                å»¶é²ç­†æ•¸=(late_mask[valid_mask], "sum")
            ).reset_index()
            
            customer_performance["é”äº¤ç‡(%)"] = (
                customer_performance["æº–æ™‚ç­†æ•¸"] / customer_performance["æœ‰æ•ˆç­†æ•¸"] * 100
            ).round(2)
            
            customer_performance = customer_performance.sort_values("é”äº¤ç‡(%)", ascending=False)
            
            # å®¢æˆ¶è¡¨ç¾åˆ†ç´š
            customer_performance["è¡¨ç¾ç­‰ç´š"] = pd.cut(
                customer_performance["é”äº¤ç‡(%)"],
                bins=[0, 80, 90, 95, 100],
                labels=["éœ€æ”¹å–„", "ä¸€èˆ¬", "è‰¯å¥½", "å„ªç§€"],
                include_lowest=True
            )
            
            st.dataframe(
                customer_performance.style.format({
                    "æœ‰æ•ˆç­†æ•¸": "{:,}",
                    "æº–æ™‚ç­†æ•¸": "{:,}",
                    "å»¶é²ç­†æ•¸": "{:,}",
                    "é”äº¤ç‡(%)": "{:.2f}%"
                }).background_gradient(subset=["é”äº¤ç‡(%)"], cmap="RdYlGn"),
                use_container_width=True
            )
            
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰å®¢æˆ¶è¡¨ç¾åˆ†æ",
                data=customer_performance.to_csv(index=False).encode("utf-8-sig"),
                file_name="å®¢æˆ¶é”äº¤è¡¨ç¾.csv",
                mime="text/csv",
            )

def enhanced_area_analysis(df: pd.DataFrame, col_map: Dict[str, Optional[str]], exclude_swi_mask: pd.Series, topn_cities: int) -> None:
    """
    å¢å¼·ç‰ˆé…é€å€åŸŸåˆ†æ
    
    Args:
        df: è³‡æ–™æ¡†
        col_map: æ¬„ä½å°æ‡‰å­—å…¸
        exclude_swi_mask: æ’é™¤SWIé®ç½©
        topn_cities: æ¯å®¢æˆ¶é¡¯ç¤ºå‰Nå€‹ç¸£å¸‚
    """
    st.markdown('<h2 class="section-header">â‘¢ é…é€å€åŸŸåˆ†æ</h2>', unsafe_allow_html=True)
    
    address_col = col_map.get("address")
    cust_name_col = col_map.get("cust_name")
    
    if not address_col or address_col not in df.columns:
        st.error("âš ï¸ æ‰¾ä¸åˆ°ã€åœ°å€ã€æ¬„ä½")
        return
    
    # è™•ç†å€åŸŸè³‡æ–™
    region_df = df[exclude_swi_mask].copy()
    region_df["ç¸£å¸‚"] = region_df[address_col].apply(enhanced_city_extraction)
    
    # çµ±è¨ˆç„¡æ³•è­˜åˆ¥çš„åœ°å€
    unknown_addresses = region_df[region_df["ç¸£å¸‚"].isna()][address_col].value_counts().head(10)
    
    region_df["ç¸£å¸‚"] = region_df["ç¸£å¸‚"].fillna("(æœªçŸ¥)")
    city_stats = region_df["ç¸£å¸‚"].value_counts().reset_index()
    city_stats.columns = ["ç¸£å¸‚", "ç­†æ•¸"]
    
    total_records = city_stats["ç­†æ•¸"].sum()
    city_stats["ä½”æ¯”(%)"] = (city_stats["ç­†æ•¸"] / total_records * 100).round(2)
    
    # å€åŸŸæŒ‡æ¨™
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        create_enhanced_metric_card("ç¸½é…é€ç­†æ•¸", f"{total_records:,}")
    with col2:
        valid_cities = len(city_stats[city_stats["ç¸£å¸‚"] != "(æœªçŸ¥)"])
        create_enhanced_metric_card("è¦†è“‹ç¸£å¸‚æ•¸", f"{valid_cities}")
    with col3:
        main_city = city_stats.iloc[0]["ç¸£å¸‚"] if not city_stats.empty else "ç„¡"
        main_count = city_stats.iloc[0]["ç­†æ•¸"] if not city_stats.empty else 0
        create_enhanced_metric_card("ä¸»è¦é…é€å€", main_city, f"{main_count}ç­†")
    with col4:
        unknown_count = city_stats[city_stats["ç¸£å¸‚"] == "(æœªçŸ¥)"]["ç­†æ•¸"].sum()
        unknown_rate = unknown_count / total_records * 100 if total_records > 0 else 0
        create_enhanced_metric_card("åœ°å€è­˜åˆ¥ç‡", f"{100-unknown_rate:.1f}%")
    
    # è©³ç´°åˆ†ææ¨™ç±¤
    tab1, tab2, tab3 = st.tabs(["ğŸ—ºï¸ ç¸£å¸‚åˆ†ä½ˆ", "ğŸ¢ å®¢æˆ¶å€åŸŸ", "âš ï¸ åœ°å€å•é¡Œ"])
    
    with tab1:
        col1, col2 = st.columns([1, 1])
        with col1:
            st.dataframe(
                city_stats.style.format({
                    "ç­†æ•¸": "{:,}",
                    "ä½”æ¯”(%)": "{:.2f}%"
                }),
                use_container_width=True,
                height=400
            )
        with col2:
            # å°ç£åœ°åœ–è¦–è¦ºåŒ– (ä½¿ç”¨é•·æ¢åœ–ä»£æ›¿)
            fig_map = px.bar(
                city_stats.head(15), 
                x="ç¸£å¸‚", 
                y="ç­†æ•¸",
                title="å‰15ç¸£å¸‚é…é€é‡",
                color="ç­†æ•¸",
                color_continuous_scale="viridis"
            )
            fig_map.update_layout(xaxis_tickangle=-45)
            st.plotly_chart(fig_map, use_container_width=True)
        
        st.download_button(
            "ğŸ“¥ ä¸‹è¼‰ç¸£å¸‚çµ±è¨ˆ",
            data=city_stats.to_csv(index=False).encode("utf-8-sig"),
            file_name="ç¸£å¸‚é…é€çµ±è¨ˆ.csv",
            mime="text/csv",
        )
    
    with tab2:
        if cust_name_col and cust_name_col in region_df.columns:
            customer_city_analysis = region_df.groupby([cust_name_col, "ç¸£å¸‚"]).size().reset_index(name="ç­†æ•¸")
            customer_city_analysis["å®¢æˆ¶ç¸½ç­†æ•¸"] = customer_city_analysis.groupby(cust_name_col)["ç­†æ•¸"].transform("sum")
            customer_city_analysis["ç¸£å¸‚ä½”æ¯”(%)"] = (
                customer_city_analysis["ç­†æ•¸"] / customer_city_analysis["å®¢æˆ¶ç¸½ç­†æ•¸"] * 100
            ).round(2)
            
            # å–æ¯å®¢æˆ¶å‰Nå€‹ç¸£å¸‚
            top_customer_cities = (
                customer_city_analysis
                .sort_values(["ç­†æ•¸"], ascending=False)
                .groupby(cust_name_col)
                .head(topn_cities)
                .sort_values([cust_name_col, "ç­†æ•¸"], ascending=[True, False])
            )
            
            st.dataframe(
                top_customer_cities.rename(columns={cust_name_col: "å®¢æˆ¶åç¨±"}).style.format({
                    "ç­†æ•¸": "{:,}",
                    "å®¢æˆ¶ç¸½ç­†æ•¸": "{:,}",
                    "ç¸£å¸‚ä½”æ¯”(%)": "{:.2f}%"
                }),
                use_container_width=True
            )
            
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰å®¢æˆ¶å€åŸŸåˆ†æ",
                data=top_customer_cities.to_csv(index=False).encode("utf-8-sig"),
                file_name=f"å®¢æˆ¶å€åŸŸåˆ†æ_Top{topn_cities}.csv",
                mime="text/csv",
            )
        else:
            st.info("ç„¡å®¢æˆ¶åç¨±æ¬„ä½ï¼Œç„¡æ³•é€²è¡Œå®¢æˆ¶å€åŸŸåˆ†æ")
    
    with tab3:
        if not unknown_addresses.empty:
            st.warning(f"ç™¼ç¾ {len(unknown_addresses)} ç¨®ç„¡æ³•è­˜åˆ¥çš„åœ°å€æ ¼å¼")
            unknown_df = unknown_addresses.reset_index()
            unknown_df.columns = ["åœ°å€", "å‡ºç¾æ¬¡æ•¸"]
            st.dataframe(unknown_df, use_container_width=True)
        else:
            st.success("âœ… æ‰€æœ‰åœ°å€éƒ½å·²æˆåŠŸè­˜åˆ¥ç¸£å¸‚")

def enhanced_loading_analysis(df: pd.DataFrame, col_map: Dict[str, Optional[str]], exclude_swi_mask: pd.Series) -> None:
    """
    å¢å¼·ç‰ˆé…é€è£è¼‰åˆ†æ
    
    Args:
        df: è³‡æ–™æ¡†
        col_map: æ¬„ä½å°æ‡‰å­—å…¸
        exclude_swi_mask: æ’é™¤SWIé®ç½©
    """
    st.markdown('<h2 class="section-header">â‘£ é…é€è£è¼‰åˆ†æ</h2>', unsafe_allow_html=True)
    
    ship_no_col = col_map.get("ship_no")
    status_col = col_map.get("status")
    
    if not ship_no_col or ship_no_col not in df.columns:
        st.error("âš ï¸ æ‰¾ä¸åˆ°ã€å‡ºåº«å–®è™Ÿã€æ¬„ä½")
        return
    
    # ç¯©é¸æ¢ä»¶
    base_mask = exclude_swi_mask.copy()
    if status_col and status_col in df.columns:
        completed_mask = df[status_col].astype(str).str.strip() == "å®Œæˆ"
        base_mask &= completed_mask
        st.info(f"âœ… å·²å¥—ç”¨ç‹€æ…‹ç¯©é¸ï¼šåƒ…é¡¯ç¤ºã€å®Œæˆã€ç‹€æ…‹çš„è¨˜éŒ„")
    else:
        st.warning("âš ï¸ æœªæ‰¾åˆ°ã€ç‹€æ…‹ã€æ¬„ä½ï¼Œå°‡é¡¯ç¤ºæ‰€æœ‰è¨˜éŒ„")
    
    loading_df = df[base_mask].copy()
    
    if loading_df.empty:
        st.warning("ç„¡ç¬¦åˆæ¢ä»¶çš„è£è¼‰æ•¸æ“š")
        return
    
    # è»Šæ¬¡ä»£ç¢¼ç”Ÿæˆ
    loading_df["è»Šæ¬¡ä»£ç¢¼"] = loading_df[ship_no_col].astype(str).str[:13]
    
    # æ•¸å€¼æ¬„ä½è™•ç†
    numeric_cols = {
        "qty": col_map.get("qty"),
        "copper_ton": col_map.get("copper_ton"),
        "fg_net_ton": col_map.get("fg_net_ton"),
        "fg_gross_ton": col_map.get("fg_gross_ton")
    }
    
    for key, col_name in numeric_cols.items():
        if col_name and col_name in loading_df.columns:
            loading_df[f"_{key}_numeric"] = pd.to_numeric(loading_df[col_name], errors="coerce")
    
    # è½‰æ›éŠ…é‡é‡ç‚ºå™¸
    if "_copper_ton_numeric" in loading_df.columns:
        loading_df["_copper_ton_converted"] = loading_df["_copper_ton_numeric"] / 1000.0
    
    # å»ºç«‹å±•ç¤ºç”¨è³‡æ–™æ¡†
    display_columns = {
        "è»Šæ¬¡ä»£ç¢¼": loading_df["è»Šæ¬¡ä»£ç¢¼"],
        "å‡ºåº«å–®è™Ÿ": loading_df[ship_no_col],
    }
    
    # å‹•æ…‹æ–°å¢å¯ç”¨æ¬„ä½
    optional_cols = {
        "DOè™Ÿ": col_map.get("do_no"),
        "æ–™è™Ÿèªªæ˜": col_map.get("item_desc"),
        "æ‰¹æ¬¡": col_map.get("lot_no"),
        "å®¢æˆ¶åç¨±": col_map.get("cust_name")
    }
    
    for display_name, col_name in optional_cols.items():
        if col_name and col_name in loading_df.columns:
            display_columns[display_name] = loading_df[col_name]
    
    # æ•¸å€¼æ¬„ä½
    numeric_display_cols = {
        "å‡ºè²¨æ•¸é‡": "_qty_numeric",
        "éŠ…é‡é‡(å™¸)": "_copper_ton_converted",
        "æˆå“æ·¨é‡(å™¸)": "_fg_net_ton_numeric",
        "æˆå“æ¯›é‡(å™¸)": "_fg_gross_ton_numeric"
    }
    
    for display_name, internal_name in numeric_display_cols.items():
        if internal_name in loading_df.columns:
            display_columns[display_name] = loading_df[internal_name].round(3)
    
    detailed_loading_df = pd.DataFrame(display_columns)
    
    # æ’åº
    sort_columns = [col for col in ["è»Šæ¬¡ä»£ç¢¼", "å‡ºåº«å–®è™Ÿ"] if col in detailed_loading_df.columns]
    if sort_columns:
        detailed_loading_df = detailed_loading_df.sort_values(sort_columns).reset_index(drop=True)
    
    # è»Šæ¬¡å½™ç¸½çµ±è¨ˆ
    summary_agg = {}
    for display_name, internal_name in numeric_display_cols.items():
        if internal_name in loading_df.columns:
            summary_agg[display_name + "å°è¨ˆ"] = (internal_name, "sum")
            summary_agg[display_name + "é …ç›®æ•¸"] = (internal_name, "count")
    
    if summary_agg:
        trip_summary = loading_df.groupby("è»Šæ¬¡ä»£ç¢¼").agg(summary_agg).round(3)
        trip_summary.columns = [col[0] for col in trip_summary.columns]
        trip_summary = trip_summary.reset_index()
    else:
        trip_summary = loading_df.groupby("è»Šæ¬¡ä»£ç¢¼").size().reset_index(name="é …ç›®æ•¸")
    
    # åˆ†ææ¨™ç±¤
    load_tab1, load_tab2, load_tab3 = st.tabs(["ğŸ“¦ è£è¼‰æ˜ç´°", "ğŸš› è»Šæ¬¡å½™ç¸½", "ğŸ“Š è£è¼‰åˆ†æ"])
    
    with load_tab1:
        st.dataframe(detailed_loading_df, use_container_width=True, height=500)
        st.download_button(
            "ğŸ“¥ ä¸‹è¼‰è£è¼‰æ˜ç´°",
            data=detailed_loading_df.to_csv(index=False).encode("utf-8-sig"),
            file_name="é…é€è£è¼‰æ˜ç´°.csv",
            mime="text/csv",
        )
    
    with load_tab2:
        # è»Šæ¬¡çµ±è¨ˆæŒ‡æ¨™
        total_trips = len(trip_summary)
        col1, col2, col3 = st.columns(3)
        with col1:
            create_enhanced_metric_card("ç¸½è»Šæ¬¡æ•¸", f"{total_trips:,}")
        with col2:
            avg_items_per_trip = detailed_loading_df.groupby("è»Šæ¬¡ä»£ç¢¼").size().mean()
            create_enhanced_metric_card("å¹³å‡æ¯è»Šé …ç›®æ•¸", f"{avg_items_per_trip:.1f}")
        with col3:
            if "éŠ…é‡é‡(å™¸)å°è¨ˆ" in trip_summary.columns:
                valid_trips = trip_summary[trip_summary["éŠ…é‡é‡(å™¸)å°è¨ˆ"] > 0]
                avg_weight = valid_trips["éŠ…é‡é‡(å™¸)å°è¨ˆ"].mean() if not valid_trips.empty else 0
                create_enhanced_metric_card("å¹³å‡è¼‰é‡(å™¸)", f"{avg_weight:.2f}")
        
        st.dataframe(
            trip_summary.style.format({col: "{:.3f}" for col in trip_summary.columns if "å°è¨ˆ" in col}),
            use_container_width=True
        )
        
        st.download_button(
            "ğŸ“¥ ä¸‹è¼‰è»Šæ¬¡å½™ç¸½",
            data=trip_summary.to_csv(index=False).encode("utf-8-sig"),
            file_name="è»Šæ¬¡å½™ç¸½çµ±è¨ˆ.csv",
            mime="text/csv",
        )
    
    with load_tab3:
        if "éŠ…é‡é‡(å™¸)å°è¨ˆ" in trip_summary.columns:
            # è¼‰é‡åˆ†ä½ˆåˆ†æ
            weight_data = trip_summary[trip_summary["éŠ…é‡é‡(å™¸)å°è¨ˆ"] > 0]["éŠ…é‡é‡(å™¸)å°è¨ˆ"]
            
            if not weight_data.empty:
                fig_dist = go.Figure()
                fig_dist.add_trace(go.Histogram(
                    x=weight_data,
                    nbinsx=20,
                    name="è¼‰é‡åˆ†ä½ˆ",
                    marker_color="lightblue"
                ))
                fig_dist.update_layout(
                    title="è»Šè¼›è¼‰é‡åˆ†ä½ˆç›´æ–¹åœ–",
                    xaxis_title="è¼‰é‡(å™¸)",
                    yaxis_title="è»Šæ¬¡æ•¸"
                )
                st.plotly_chart(fig_dist, use_container_width=True)
                
                # è¼‰é‡çµ±è¨ˆæ‘˜è¦
                stats_summary = pd.DataFrame({
                    "çµ±è¨ˆé …ç›®": ["æœ€å°è¼‰é‡", "æœ€å¤§è¼‰é‡", "å¹³å‡è¼‰é‡", "ä¸­ä½æ•¸è¼‰é‡", "æ¨™æº–å·®"],
                    "æ•¸å€¼(å™¸)": [
                        weight_data.min(),
                        weight_data.max(),
                        weight_data.mean(),
                        weight_data.median(),
                        weight_data.std()
                    ]
                }).round(3)
                
                st.dataframe(stats_summary, use_container_width=True)
        else:
            st.info("ç„¡éŠ…é‡é‡æ•¸æ“šå¯ä¾›è¼‰é‡åˆ†æ")

# ========================
# æ™ºèƒ½æ¬„ä½å°æ‡‰ä»‹é¢
# ========================

def display_column_mapping_interface(df: pd.DataFrame, auto_mapping: Dict[str, Optional[str]]) -> Dict[str, Optional[str]]:
    """
    é¡¯ç¤ºæ™ºèƒ½æ¬„ä½å°æ‡‰ä»‹é¢
    
    Args:
        df: è³‡æ–™æ¡†
        auto_mapping: è‡ªå‹•åµæ¸¬çš„æ¬„ä½å°æ‡‰
        
    Returns:
        Dict: ç”¨æˆ¶ç¢ºèªå¾Œçš„æ¬„ä½å°æ‡‰
    """
    st.markdown('<h2 class="section-header">ğŸ”§ æ¬„ä½å°æ‡‰è¨­å®š</h2>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("**è‡ªå‹•åµæ¸¬çµæœ**")
        mapping_status = []
        for logical_name, detected_col in auto_mapping.items():
            status = "âœ… å·²åµæ¸¬" if detected_col else "âŒ æœªåµæ¸¬"
            mapping_status.append({
                "é‚è¼¯æ¬„ä½": logical_name,
                "åµæ¸¬åˆ°çš„æ¬„ä½": detected_col or "(ç„¡)",
                "ç‹€æ…‹": status
            })
        
        st.dataframe(pd.DataFrame(mapping_status), use_container_width=True)
    
    with col2:
        st.markdown("**æ‰‹å‹•èª¿æ•´**")
        
        # æ¬„ä½é¸é …
        column_options = ["(ä¸ä½¿ç”¨)"] + list(df.columns)
        
        # é—œéµæ¬„ä½æ˜ å°„
        key_fields = {
            "ship_type": "å‡ºè²¨ç”³è«‹é¡å‹",
            "due_date": "æŒ‡å®šåˆ°è²¨æ—¥æœŸ", 
            "sign_date": "å®¢æˆ¶ç°½æ”¶æ—¥æœŸ",
            "address": "åœ°å€",
            "cust_name": "å®¢æˆ¶åç¨±"
        }
        
        user_mapping = {}
        for field_key, field_display in key_fields.items():
            default_idx = 0
            if auto_mapping.get(field_key) in column_options:
                default_idx = column_options.index(auto_mapping.get(field_key))
            
            selected = st.selectbox(
                f"{field_display}:",
                column_options,
                index=default_idx,
                key=f"map_{field_key}"
            )
            user_mapping[field_key] = selected if selected != "(ä¸ä½¿ç”¨)" else None
        
        # å…¶ä»–æ¬„ä½æ‘ºç–Šé¡¯ç¤º
        with st.expander("å…¶ä»–æ¬„ä½è¨­å®š"):
            other_fields = {
                "cust_id": "å®¢æˆ¶ç·¨è™Ÿ",
                "copper_ton": "éŠ…é‡é‡",
                "qty": "å‡ºè²¨æ•¸é‡",
                "ship_no": "å‡ºåº«å–®è™Ÿ",
                "do_no": "DOè™Ÿ",
                "item_desc": "æ–™è™Ÿèªªæ˜",
                "lot_no": "æ‰¹æ¬¡",
                "fg_net_ton": "æˆå“æ·¨é‡",
                "fg_gross_ton": "æˆå“æ¯›é‡",
                "status": "ç‹€æ…‹"
            }
            
            for field_key, field_display in other_fields.items():
                default_idx = 0
                if auto_mapping.get(field_key) in column_options:
                    default_idx = column_options.index(auto_mapping.get(field_key))
                
                selected = st.selectbox(
                    f"{field_display}:",
                    column_options,
                    index=default_idx,
                    key=f"map_{field_key}_other"
                )
                user_mapping[field_key] = selected if selected != "(ä¸ä½¿ç”¨)" else None
    
    return user_mapping

# ========================
# ä¸»ç¨‹å¼æµç¨‹
# ========================

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    
    # æª”æ¡ˆä¸Šå‚³å€
    st.markdown("### ğŸ“ æª”æ¡ˆä¸Šå‚³")
    uploaded_file = st.file_uploader(
        "é¸æ“‡æ‚¨çš„æ•¸æ“šæª”æ¡ˆ",
        type=["xlsx", "xls", "csv"],
        help="æ”¯æ´æ ¼å¼ï¼šExcel (.xlsx, .xls) å’Œ CSV (.csv)ï¼Œæª”æ¡ˆå¤§å°é™åˆ¶ 200MB",
        accept_multiple_files=False
    )
    
    if not uploaded_file:
        # é¡¯ç¤ºä½¿ç”¨èªªæ˜
        st.markdown("### ğŸ“– ä½¿ç”¨èªªæ˜")
        st.info("""
        **åŠŸèƒ½æ¦‚è¿°ï¼š**
        - ğŸ“Š **å‡ºè²¨é¡å‹åˆ†æ**ï¼šçµ±è¨ˆå„é¡å‹ç­†æ•¸èˆ‡éŠ…é‡é‡
        - ğŸšš **é”äº¤ç‡åˆ†æ**ï¼šè¨ˆç®—æº–æ™‚äº¤ä»˜ç‡ï¼Œè­˜åˆ¥å»¶é²å•é¡Œ  
        - ğŸ—ºï¸ **å€åŸŸåˆ†æ**ï¼šåˆ†æé…é€åœ°å€åˆ†ä½ˆèˆ‡å®¢æˆ¶åå¥½
        - ğŸ“¦ **è£è¼‰åˆ†æ**ï¼šè»Šæ¬¡è£è¼‰çµ±è¨ˆèˆ‡æ•ˆç‡åˆ†æ
        
        **é–‹å§‹ä½¿ç”¨ï¼š**
        1. ä¸Šå‚³åŒ…å«å‡ºè²¨æ•¸æ“šçš„ Excel æˆ– CSV æª”æ¡ˆ
        2. ç³»çµ±å°‡è‡ªå‹•åµæ¸¬æ¬„ä½ä¸¦é–‹å§‹åˆ†æ
        3. ä½¿ç”¨å´é‚Šæ¬„é€²è¡Œç¯©é¸å’Œå®¢è£½åŒ–åˆ†æ
        """)
        return
    
    # è¼‰å…¥è³‡æ–™
    file_extension = uploaded_file.name.split(".")[-1].lower()
    
    with st.spinner("æ­£åœ¨è™•ç†æª”æ¡ˆ..."):
        df = load_data(uploaded_file, file_extension)
    
    if df.empty:
        st.error("æª”æ¡ˆè¼‰å…¥å¤±æ•—æˆ–æª”æ¡ˆç‚ºç©ºï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼")
        return
    
    # æˆåŠŸè¼‰å…¥æç¤º
    st.success(f"âœ… æˆåŠŸè¼‰å…¥ {len(df):,} ç­†è¨˜éŒ„ï¼Œ{len(df.columns)} å€‹æ¬„ä½")
    
    # è‡ªå‹•åµæ¸¬æ¬„ä½å°æ‡‰
    auto_detected_mapping = smart_column_detector(df.columns.tolist())
    
    # å»ºç«‹ä¸»è¦é ç±¤
    main_tab1, main_tab2, main_tab3 = st.tabs(["ğŸ“Š æ•¸æ“šåˆ†æ", "âš™ï¸ æ¬„ä½è¨­å®š", "ğŸ“„ åŸå§‹æ•¸æ“š"])
    
    with main_tab3:
        st.markdown("### åŸå§‹æ•¸æ“šé è¦½")
        st.dataframe(df, use_container_width=True, height=600)
        
        # åŸºæœ¬çµ±è¨ˆè³‡è¨Š
        st.markdown("### ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ")
        basic_stats = pd.DataFrame({
            "é …ç›®": ["ç¸½è¨˜éŒ„æ•¸", "æ¬„ä½æ•¸", "ç©ºå€¼ç¸½æ•¸", "é‡è¤‡è¨˜éŒ„æ•¸"],
            "æ•¸å€¼": [
                len(df),
                len(df.columns),
                df.isnull().sum().sum(),
                df.duplicated().sum()
            ]
        })
        st.dataframe(basic_stats, use_container_width=True)
    
    with main_tab2:
        final_column_mapping = display_column_mapping_interface(df, auto_detected_mapping)
    
    with main_tab1:
        # ä½¿ç”¨æœ€çµ‚çš„æ¬„ä½å°æ‡‰
        final_column_mapping = auto_detected_mapping  # ç°¡åŒ–ç‰ˆï¼šç›´æ¥ä½¿ç”¨è‡ªå‹•åµæ¸¬
        
        # å´é‚Šæ¬„ç¯©é¸æ§åˆ¶
        with st.sidebar:
            st.markdown("### âš™ï¸ åˆ†ææ§åˆ¶é¢æ¿")
            
            # é€šç”¨ç¯©é¸
            with st.expander("ğŸ” é€šç”¨ç¯©é¸", expanded=True):
                selected_ship_types = []
                if final_column_mapping.get("ship_type"):
                    unique_types = sorted(df[final_column_mapping["ship_type"]].dropna().unique())
                    selected_ship_types = st.multiselect(
                        "å‡ºè²¨ç”³è«‹é¡å‹", 
                        options=unique_types,
                        default=[],
                        key="filter_ship_types"
                    )
                
                selected_customers = []
                if final_column_mapping.get("cust_name"):
                    unique_customers = sorted(df[final_column_mapping["cust_name"]].dropna().unique())
                    selected_customers = st.multiselect(
                        "å®¢æˆ¶åç¨±",
                        options=unique_customers[:100],  # é™åˆ¶é¡¯ç¤ºæ•¸é‡é¿å…ä»‹é¢éè¼‰
                        default=[],
                        key="filter_customers"
                    )
                
                # æ—¥æœŸç¯„åœç¯©é¸
                date_filter_range = None
                if final_column_mapping.get("due_date"):
                    due_date_series = safe_datetime_convert(df[final_column_mapping["due_date"]])
                    if due_date_series.notna().any():
                        min_date = due_date_series.min().date()
                        max_date = due_date_series.max().date()
                        date_filter_range = st.date_input(
                            "æŒ‡å®šåˆ°è²¨æ—¥æœŸç¯„åœ",
                            value=(min_date, max_date),
                            min_value=min_date,
                            max_value=max_date,
                            key="date_range_filter"
                        )
            
            # é€²éšè¨­å®š
            with st.expander("âš™ï¸ é€²éšè¨­å®š", expanded=False):
                topn_cities_setting = st.slider(
                    "æ¯å®¢æˆ¶é¡¯ç¤ºå‰Nå€‹é…é€ç¸£å¸‚", 
                    min_value=1, 
                    max_value=10, 
                    value=3,
                    key="topn_cities_slider"
                )
                
                enable_detailed_analysis = st.checkbox(
                    "å•Ÿç”¨è©³ç´°åˆ†ææ¨¡å¼", 
                    value=True,
                    help="åŒ…å«æ›´å¤šçµ±è¨ˆåœ–è¡¨å’Œæ·±åº¦åˆ†æ"
                )
        
        # å¥—ç”¨ç¯©é¸æ¢ä»¶
        filtered_data = df.copy()
        
        if selected_ship_types and final_column_mapping.get("ship_type"):
            filtered_data = filtered_data[filtered_data[final_column_mapping["ship_type"]].isin(selected_ship_types)]
        
        if selected_customers and final_column_mapping.get("cust_name"):
            filtered_data = filtered_data[filtered_data[final_column_mapping["cust_name"]].isin(selected_customers)]
        
        if date_filter_range and len(date_filter_range) == 2 and final_column_mapping.get("due_date"):
            start_date = pd.to_datetime(date_filter_range[0])
            end_date = pd.to_datetime(date_filter_range[1])
            due_dates = safe_datetime_convert(filtered_data[final_column_mapping["due_date"]])
            date_mask = (due_dates >= start_date) & (due_dates <= end_date)
            filtered_data = filtered_data[date_mask]
        
        # é¡¯ç¤ºç¯©é¸çµæœ
        if len(filtered_data) != len(df):
            st.info(f"ğŸ” ç¯©é¸å¾Œé¡¯ç¤º {len(filtered_data):,} ç­†è¨˜éŒ„ï¼ˆåŸå§‹ï¼š{len(df):,} ç­†ï¼‰")
        
        # SWIæ’é™¤é‚è¼¯
        exclude_swi_condition = pd.Series(True, index=filtered_data.index)
        if final_column_mapping.get("ship_type"):
            exclude_swi_condition = filtered_data[final_column_mapping["ship_type"]] != "SWI-å¯„åº«"
        
        # åŸ·è¡Œå„é …åˆ†æ
        if not filtered_data.empty:
            enhanced_shipment_analysis(filtered_data, final_column_mapping)
            st.markdown("---")
            enhanced_delivery_performance(filtered_data, final_column_mapping, exclude_swi_condition)
            st.markdown("---") 
            enhanced_area_analysis(filtered_data, final_column_mapping, exclude_swi_condition, topn_cities_setting)
            st.markdown("---")
            enhanced_loading_analysis(filtered_data, final_column_mapping, exclude_swi_condition)
        else:
            st.warning("âš ï¸ ç¯©é¸æ¢ä»¶éæ–¼åš´æ ¼ï¼Œç„¡æ•¸æ“šå¯é¡¯ç¤º")

# ========================
# ç¨‹å¼å…¥å£
# ========================

if __name__ == "__main__":
    main()
